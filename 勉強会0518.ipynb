{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非線形SVMについての掘り下げ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １: カーネル関数とは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用語について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ カーネル関数とは、x_nとx_iの近さを表す関数 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$k(\\boldsymbol{x},\\boldsymbol{x'}) = φ(\\boldsymbol{x})^{\\mathrm{T}}φ(\\boldsymbol{x'}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ = φ_1(\\boldsymbol{x'})φ_1(\\boldsymbol{x'})+φ_2(\\boldsymbol{x'})φ_2(\\boldsymbol{x'})+ ... +φ_n(\\boldsymbol{x'})φ_n(\\boldsymbol{x'}) で表される　$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$　特徴空間とは、入力ベクトル\\boldsymbol{x}の写像 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### カーネル関数の定義方法二種類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "１、特徴空間への写像を考え、その内積をカーネル関数とする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k(\\boldsymbol{x},\\boldsymbol{x'}) = φ(\\boldsymbol{x})^{\\mathrm{T}}φ(\\boldsymbol{x'}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "２、カーネル関数を直接定義する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k(\\boldsymbol{x},\\boldsymbol{z}) = (\\boldsymbol{x}^{\\mathrm{T}}\\boldsymbol{z})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$二次元入力(x_1,x_2)を考え、展開すると$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "k(\\boldsymbol{x},\\boldsymbol{x'}) &= (x_1z_1+ x_2z_2)^2 \\\\\n",
    "&= x_1^2z_1^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2\\\\\n",
    "&= (x_1^2,\\sqrt{2}x_1x_2,x_2^2)(z_1^2,\\sqrt{2}z_1z_2,z_2^2)\\\\\n",
    "&= φ(\\boldsymbol{x})^{\\mathrm{T}}φ(\\boldsymbol{z})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは特徴ベクトルの積で表されたので、有効なカーネル関数\n",
    "$$ちなみに、有効になるための必要十分条件は任意の{x_n}に対して要素がk(x_n,x_n)で与えられるグラム行列kが半正定値であること$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２: Gaussian Kernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "e^{-γ|x_i-x_j|^2}\\\\\n",
    "&=e^{-γ(x_i-x_j)^2}\\\\\n",
    "&=e^{-γx_i^2+2γx_ix_j-γx_j^2}\\\\\n",
    "&=e^{-γx_i^2-γx_j^2}e^{2γx_ix_j}\\\\\n",
    "&=e^{-γx_i^2-γx_j^2}(1+\\frac{2γx_ix_j}{1!}+\\frac{(2γx_ix_j)^2}{2!}+...)\\\\\n",
    "&=e^{-γx_i^2-γx_j^2}(1+\\sqrt{\\frac{2γ}{1!}}x_i\\sqrt{\\frac{2γ}{1!}x_j}+...)\\\\\n",
    "&= φ(\\boldsymbol{x_i})^{\\mathrm{T}}φ(\\boldsymbol{x_j})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なぜカーネル関数を使うとうまくいくか\n",
    "φという特徴空間への写像を自由に選べるから\n",
    "\n",
    "例えばgaussian kernel だと、特徴空間の次元が無限になり、超平面（線形関数）で分割できるようになる。\n",
    "また、カーネル関数を用いると、φがわからずとも内積を計算できる。\n",
    "\n",
    "この前は、「線形分離がおおよそできる」という条件を満たす任意のφについて成り立つ式変形をしていた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ３: ソフトマージン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マージンの外側の領域を0、正しく分類されているがマージン内のものを0〜1、誤分類を1とする、スラック変数を定義する。\n",
    "$$C\\sum_{k=1}^{n} スラック変数 + \\frac{1}{2}|\\boldsymbol{w}|^2 \\\\ $$\n",
    "Cの値を無限大にすれば、ハードマージンとなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 非線形SVM（ソフトマージン）\n",
    "# cvxoptのQuadratic Programmingを解く関数を使用\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "from pylab import *\n",
    "\n",
    "N = 200         # データ数\n",
    "C = 0.5         # スラック変数を用いて表されるペナルティとマージンのトレードオフパラメータ\n",
    "SIGMA = 0.3     # ガウスカーネルのパラメータ\n",
    "\n",
    "\n",
    "# ガウスカーネル\n",
    "def gaussian_kernel(x, y):\n",
    "    return np.exp(-norm(x-y)**2 / (2 * (SIGMA ** 2)))\n",
    "\n",
    "# どちらのカーネル関数を使うかここで指定\n",
    "kernel = gaussian_kernel\n",
    "\n",
    "def f(x, a, t, X, b):\n",
    "    sum = 0.0\n",
    "    for n in range(N):\n",
    "        sum += a[n] * t[n] * kernel(x, X[n])\n",
    "    return sum + b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 訓練データをロード\n",
    "    data = np.genfromtxt(\"classification.txt\")\n",
    "    X = data[:,0:2]\n",
    "    t = data[:,2] * 2 - 1.0  # 教師信号を-1 or 1に変換\n",
    "    \n",
    "    # ラグランジュ乗数を二次計画法（Quadratic Programming）で求める\n",
    "    K = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i, j] = t[i] * t[j] * kernel(X[i], X[j])\n",
    "    \n",
    "    Q = cvxopt.matrix(K)\n",
    "    p = cvxopt.matrix(-np.ones(N))\n",
    "    temp1 = np.diag([-1.0]*N)\n",
    "    temp2 = np.identity(N)\n",
    "    G = cvxopt.matrix(np.vstack((temp1, temp2)))\n",
    "    temp1 = np.zeros(N)\n",
    "    temp2 = np.ones(N) * C\n",
    "    h = cvxopt.matrix(np.hstack((temp1, temp2)))\n",
    "    A = cvxopt.matrix(t, (1,N))\n",
    "    b = cvxopt.matrix(0.0)\n",
    "    sol = cvxopt.solvers.qp(Q, p, G, h, A, b)\n",
    "    a = array(sol['x']).reshape(N)\n",
    "    print (a)\n",
    "    \n",
    "    # サポートベクトルのインデックスを抽出\n",
    "    S = []\n",
    "    M = []\n",
    "    for n in range(len(a)):\n",
    "        if 0 < a[n]:\n",
    "            S.append(n)\n",
    "        if 0 < a[n] < C:\n",
    "            M.append(n)\n",
    "    \n",
    "    # bを計算\n",
    "    sum = 0\n",
    "    for n in M:\n",
    "        temp = 0\n",
    "        for m in S:\n",
    "            temp += a[m] * t[m] * kernel(X[n], X[m])\n",
    "        sum += (t[n] - temp)\n",
    "    b = sum / len(M)\n",
    "    \n",
    "    print (S, b)\n",
    "    \n",
    "    # 訓練データを描画\n",
    "    for n in range(N):\n",
    "        if t[n] > 0:\n",
    "            scatter(X[n,0], X[n,1], c='b', marker='o')\n",
    "        else:\n",
    "            scatter(X[n,0], X[n,1], c='r', marker='o')\n",
    "    \n",
    "    # サポートベクトルを描画\n",
    "#    for n in S:\n",
    "#        scatter(X[n,0], X[n,1], s=80, c='c', marker='o')\n",
    "    \n",
    "    # 識別境界を描画\n",
    "    X1, X2 = meshgrid(linspace(-2,2,50), linspace(-2,2,50))\n",
    "    w, h = X1.shape\n",
    "    X1.resize(X1.size)\n",
    "    X2.resize(X2.size)\n",
    "    Z = array([f(array([x1, x2]), a, t, X, b) for (x1, x2) in zip(X1, X2)])\n",
    "    X1.resize((w, h))\n",
    "    X2.resize((w, h))\n",
    "    Z.resize((w, h))\n",
    "    CS = contour(X1, X2, Z, [0.0], colors='k', linewidths=1, origin='lower')\n",
    "    \n",
    "    for n in S:\n",
    "        print (f(X[n], a, t, X, b))\n",
    "    \n",
    "    xlim(-2, 2)\n",
    "    ylim(-2, 2)\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 台湾大学資料　https://www.csie.ntu.edu.tw/~cjlin/talks/kuleuven_svm.pdf\n",
    "http://aidiary.hatenablog.com/entry/20100503/1272889097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
